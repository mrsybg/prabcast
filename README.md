# PrABCast - AI-gestÃ¼tzte Absatz- und Bedarfsprognose

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.31+-red.svg)](https://streamlit.io)

> **Forschungsprojekt** des [RIF Institut fÃ¼r Forschung und Transfer](https://www.rif-ev.de) in Kooperation mit dem [Institut fÃ¼r Produktionssysteme (IPS)](https://ips.mb.tu-dortmund.de) der TU Dortmund.

---

## âš ï¸ Demonstrator-Hinweis

**Diese Anwendung ist ein wissenschaftlicher Demonstrator** zur Veranschaulichung moderner Prognoseverfahren und deren praktischer Anwendbarkeit in der Produktion und im Supply Chain Management.

**Vereinfachungen fÃ¼r Benutzerfreundlichkeit:**
- Modellparametrisierung basiert auf **heuristischen Standardwerten**
- Hyperparameter-Optimierung ist bewusst vereinfacht
- Fokus liegt auf intuitiver Bedienbarkeit und schnellen Ergebnissen

**FÃ¼r produktive Anwendungen** bietet das RIF Institut vollstÃ¤ndige Implementierungen mit:
- Umfassender Hyperparameter-Optimierung (Grid Search, Bayesian Optimization)
- Erweiterter Kreuzvalidierung und Modellselektion
- DomÃ¤nenspezifischem Feature Engineering
- MaÃŸgeschneiderten PrognoselÃ¶sungen

ğŸ“§ **Kontakt:** [info@rif-ev.de](mailto:info@rif-ev.de)

---

## ğŸ¯ Ãœberblick

**PrABCast** unterstÃ¼tzt Unternehmen dabei, Maschinelle Lernverfahren in der Absatz- und Bedarfsprognose einzusetzen. Die Anwendung kombiniert klassische statistische Verfahren mit modernen ML-Algorithmen in einer interaktiven Streamlit-OberflÃ¤che.

### Hauptfunktionen

- **ğŸ“Š ABC/XYZ-Analyse** â€“ Produktklassifikation nach Wert und VariabilitÃ¤t
- **ğŸ“ˆ Univariate Prognosen** â€“ ARIMA, SARIMA, Prophet, LSTM, XGBoost
- **ğŸ”— Multivariate Modelle** â€“ Integration externer Einflussfaktoren (Wirtschaftsdaten, Indizes)
- **ğŸ“‰ Statistische Analysen** â€“ StationaritÃ¤tstests, Zeitreihenzerlegung, Korrelationen
- **ğŸ¨ Interaktive Visualisierungen** â€“ Plotly-basierte Dashboards
- **ğŸ“ Datenhandling** â€“ CSV-Import/Export, flexible Aggregation

---

## ğŸš€ Schnellstart

### Installation

```bash
# Repository klonen
git clone https://github.com/mrsybg/prabcast.git
cd prabcast

# Virtual Environment erstellen
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows

# Dependencies installieren
pip install -r requirements.txt

# Anwendung starten
streamlit run app/run.py
```

Die Anwendung Ã¶ffnet sich automatisch unter `http://localhost:8501`

### Externe APIs (optional)

FÃ¼r multivariate Prognosen mit Wirtschaftsdaten:

1. Erstelle eine `.env` Datei im Hauptverzeichnis
2. FÃ¼ge deinen [FRED API-Key](https://fred.stlouisfed.org/docs/api/api_key.html) hinzu:
   ```
   FRED_API_KEY=your_key_here
   ```

Siehe [SETUP.md](SETUP.md) fÃ¼r detaillierte Installationsanweisungen.

---

## ğŸ“š Dokumentation

- **[SETUP.md](SETUP.md)** â€“ AusfÃ¼hrliche Installationsanleitung
- **[QUICKSTART.md](QUICKSTART.md)** â€“ 5-Minuten-Schnelleinstieg
- **[GITHUB_SECURITY_UPDATE.md](GITHUB_SECURITY_UPDATE.md)** â€“ SicherheitsmaÃŸnahmen und API-Key-Externalisierung

---

## ğŸ—ï¸ Architektur

```
prabcast/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ layout.py          # Haupt-UI und Tab-Struktur
â”‚   â”œâ”€â”€ run.py             # Einstiegspunkt
â”‚   â”œâ”€â”€ models.py          # Univariate Modelle
â”‚   â””â”€â”€ models_multi.py    # Multivariate Modelle
â”œâ”€â”€ tabs/
â”‚   â”œâ”€â”€ upload.py          # Datenimport
â”‚   â”œâ”€â”€ forecast.py        # Modellvergleich
â”‚   â”œâ”€â”€ advanced_forecast.py  # Datenanreicherung
â”‚   â”œâ”€â”€ multivariate_forecast.py  # Multivariate Prognosen
â”‚   â””â”€â”€ glossar.py         # Fachbegriffe-Glossar
â”œâ”€â”€ setup_module/
â”‚   â”œâ”€â”€ helpers.py         # Zentrale Hilfsfunktionen
â”‚   â”œâ”€â”€ evaluation.py      # Metriken (sMAPE, MAE, RMSE)
â”‚   â””â”€â”€ model_registry.py  # Modellverwaltung
â””â”€â”€ templates/
    â””â”€â”€ custom_data_template.csv  # CSV-Vorlage
```

---

## ğŸ¤ Mitwirken

BeitrÃ¤ge sind willkommen! Siehe [CONTRIBUTING.md](CONTRIBUTING.md) fÃ¼r:
- Code-Konventionen
- Pull-Request-Prozess
- Entwickler-Richtlinien

---

## ğŸ“„ Lizenz

Dieses Projekt ist unter der [MIT License](LICENSE) lizenziert.

---

## ğŸ”— Links

- **Projektwebsite:** [IPS Forschungsprojekte - PrABCast](https://ips.mb.tu-dortmund.de/forschen-beraten/forschungsprojekte/prabcast/)
- **RIF Institut:** [www.rif-ev.de](https://www.rif-ev.de)
- **Kontakt:** [info@rif-ev.de](mailto:info@rif-ev.de)

---

## ğŸ™ Danksagungen

Entwickelt im Rahmen eines Forschungsprojekts in Zusammenarbeit mit:
- RIF Institut fÃ¼r Forschung und Transfer e.V.
- Institut fÃ¼r Produktionssysteme (IPS), TU Dortmund
- GefÃ¶rdert durch [FÃ¶rderprogramm]

---

*Letzte Aktualisierung: Oktober 2024 | Version 2.0*

### Session State Variablen

Die Anwendung nutzt Streamlit's Session State fÃ¼r persistente Datenhaltung zwischen Tab-Wechseln:

```python
# Basis-Datenstrukturen
st.session_state.df                     # Hauptdatensatz (DataFrame)
st.session_state.date_column           # AusgewÃ¤hlte Datumsspalte
st.session_state.selected_products_in_data  # AusgewÃ¤hlte Produktspalten
st.session_state.start_date            # Globales Startdatum
st.session_state.end_date              # Globales Enddatum

# Verarbeitungsstatus
st.session_state.ready_for_processing  # Boolean: Daten bereit fÃ¼r Analyse

# Erweiterte Daten (fÃ¼r multivariate Prognosen)
st.session_state.multivariate_data     # Angereicherte Daten mit externen Faktoren

# Modell-Persistierung
st.session_state.saved_models_for_complex  # Gespeicherte trainierte Modelle
st.session_state.forecast_complex_results  # Prognoseergebnisse
```

### Datenfluss zwischen Tabs

```
1. UPLOAD TAB
   â”œâ”€â”€ CSV-Import â†’ st.session_state.df
   â”œâ”€â”€ Datumsspalte wÃ¤hlen â†’ st.session_state.date_column
   â”œâ”€â”€ Produkte wÃ¤hlen â†’ st.session_state.selected_products_in_data
   â””â”€â”€ ready_for_processing = True

2. ABSATZANALYSE TABS
   â”œâ”€â”€ Verwenden st.session_state.df
   â”œâ”€â”€ Nutzen selected_products_in_data
   â””â”€â”€ Arbeiten mit gefilterten Datumsbereichen

3. MODELLVERGLEICH (FORECAST TAB)
   â”œâ”€â”€ Univariate Modelle mit st.session_state.df
   â””â”€â”€ Performance-Vergleiche

4. DATENANREICHERUNG (ADVANCED_FORECAST TAB)
   â”œâ”€â”€ LÃ¤dt externe Daten (Yahoo Finance, FRED)
   â”œâ”€â”€ FÃ¼hrt Korrelationsanalysen durch
   â””â”€â”€ Speichert â†’ st.session_state.multivariate_data

5. MULTIVARIATE PROGNOSE TAB
   â”œâ”€â”€ Nutzt st.session_state.multivariate_data
   â”œâ”€â”€ Trainiert komplexe Modelle (LSTM, XGBoost)
   â””â”€â”€ Speichert Modelle â†’ st.session_state.saved_models_for_complex

6. KOMPLEXE PROGNOSE TAB
   â”œâ”€â”€ Nutzt gespeicherte Modelle
   â””â”€â”€ Generiert finale Prognosen
```

---

## Implementierte Algorithmen und Modelle

### Univariate Zeitreihenmodelle (`app/models.py`)

#### 1. **Statistische Modelle**
```python
class ARIMAModel:
    # Autoregressive Integrated Moving Average
    # Parameter: order=(p,d,q)
    # Geeignet fÃ¼r: Trend- und AR-Komponenten

class SARIMAXModel:
    # Seasonal ARIMA with eXogenous variables
    # Parameter: order=(p,d,q), seasonal_order=(P,D,Q,s)
    # Geeignet fÃ¼r: Saisonale Zeitreihen

class ProphetModel:
    # Facebook Prophet - Additive Zeitreihenmodell
    # Automatische Trend- und SaisonalitÃ¤tserkennung
    # Geeignet fÃ¼r: GeschÃ¤ftsdaten mit starker SaisonalitÃ¤t
```

#### 2. **GlÃ¤ttungsverfahren**
```python
class SimpleExpSmoothing:
    # Einfache exponentielle GlÃ¤ttung
    # Geeignet fÃ¼r: Daten ohne Trend/SaisonalitÃ¤t

class ExponentialSmoothing:
    # Holt-Winters Verfahren
    # BerÃ¼cksichtigt Trend und SaisonalitÃ¤t
    # Geeignet fÃ¼r: Komplexe saisonale Muster
```

#### 3. **Naive Basismethoden**
```python
class SeasonalNaiveModel:
    # Wiederholung der letzten Saison
    # Parameter: season_length=12
    # Geeignet fÃ¼r: Baseline-Vergleiche

class MovingAverageModel:
    # Gleitender Durchschnitt
    # Parameter: window=12
    # Geeignet fÃ¼r: Einfache Trend-SchÃ¤tzung
```

#### 4. **Machine Learning Modelle**
```python
class LSTMModel:
    # Long Short-Term Memory Neural Network
    # Automatische Feature-Extraktion
    # Geeignet fÃ¼r: Komplexe nichtlineare Muster

class GRUModel:
    # Gated Recurrent Unit
    # Effizientere Alternative zu LSTM
    # Geeignet fÃ¼r: Mittlere KomplexitÃ¤t

class XGBoostModel:
    # Gradient Boosting Decision Trees
    # Feature Engineering erforderlich
    # Geeignet fÃ¼r: Strukturierte Zeitreihendaten

class RandomForestModel:
    # Ensemble von EntscheidungsbÃ¤umen
    # Robuste Vorhersagen
    # Geeignet fÃ¼r: Rauschige Daten
```

### Multivariate Modelle (`app/models_multi.py`)

#### 1. **Deep Learning Architekturen**
```python
def build_lstm_model(input_shape):
    model = Sequential([
        LSTM(64, return_sequences=True),
        Dropout(0.2),
        LSTM(32),
        Dropout(0.2),
        Dense(16, activation='relu'),
        Dense(1)
    ])
    # Geeignet fÃ¼r: Multivariate Zeitreihen mit komplexen AbhÃ¤ngigkeiten
```

#### 2. **Gradient Boosting**
```python
def build_xgboost_model():
    return XGBRegressor(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=7,
        subsample=0.8,
        colsample_bytree=0.8
    )
    # Geeignet fÃ¼r: Strukturierte multivariate Daten
```

### Evaluationsmetriken (`setup_module/evaluation.py`)

```python
# PrognosegÃ¼te-Metriken
- sMAPE (Symmetric Mean Absolute Percentage Error)
- MAE (Mean Absolute Error) 
- RMSE (Root Mean Square Error)
- MASE (Mean Absolute Scaled Error)

# Performance-Metriken
- Trainingszeit (Sekunden)
- Speicherverbrauch (MB)
- CPU-Auslastung
```

---

## Tab-spezifische FunktionalitÃ¤ten

### 1. **Upload Tab** (`tabs/upload.py`)
```python
Funktionen:
- CSV-Datei-Import mit automatischer Encoding-Erkennung
- Flexible Datumserkennung (DD.MM.YYYY Format)
- Multi-Produkt-Auswahl
- Datumsbereich-Definition
- Datenvalidierung und -preprocessing

Session State Updates:
- st.session_state.df
- st.session_state.date_column
- st.session_state.selected_products_in_data
- st.session_state.ready_for_processing
```

### 2. **Rohdaten Tab** (`tabs/rohdaten.py`)
```python
Funktionen:
- Deskriptive Statistiken (count, mean, std, min, max, quartile)
- Zeitreihenvisualisierung mit Plotly
- Flexibler Datumsfilter
- Aggregationsoptionen (monatlich, quartalsweise, jÃ¤hrlich)
- Deutsche Zahlenformatierung

Visualisierungen:
- Liniendiagramme fÃ¼r Zeitreihen
- Interaktive Zoom- und Pan-Funktionen
```

### 3. **Aggregation Tab** (`tabs/aggregation.py`)
```python
Funktionen:
- Multi-Level-Aggregation (Monat/Quartal/Jahr)
- Gesamtabsatz-Berechnung Ã¼ber alle Produkte
- Vergleichende Visualisierungen
- Flexible Datumsfilterung

Aggregationsmethoden:
- Summe (fÃ¼r Absatzmengen)
- Durchschnitt (fÃ¼r normalisierte Vergleiche)
```

### 4. **ABC-XYZ Analyse** (`tabs/abcxyz.py`)
```python
Klassifikationskriterien:
- ABC: Basierend auf kumulativem Anteil am Gesamtabsatz
  - A: 0-80% (hoher Wert)
  - B: 80-95% (mittlerer Wert)  
  - C: 95-100% (niedriger Wert)

- XYZ: Basierend auf Variationskoeffizient
  - X: CV < 22% (konstante Nachfrage)
  - Y: 22% â‰¤ CV < 50% (schwankende Nachfrage)
  - Z: CV â‰¥ 50% (unregelmÃ¤ÃŸige Nachfrage)

Ausgabe:
- 9-Felder-Matrix Visualisierung
- Produktklassifikation mit Empfehlungen
- Export der Klassifikationsergebnisse
```

### 5. **STL-Zerlegung** (`tabs/zerlegung.py`)
```python
STL-Dekomposition (Seasonal-Trend decomposition using Loess):
- Trendkomponente: Langfristige Entwicklung
- Saisonkomponente: Wiederkehrende Muster
- Restkomponente: UnerklÃ¤rbarer Rest und AusreiÃŸer

Parameter:
- Automatische Saisonperioden-Erkennung
- Robuste Loess-GlÃ¤ttung
- Interaktive 4-Panel-Visualisierung
```

### 6. **Statistische Tests** (`tabs/statistische_tests.py`)
```python
Implementierte Tests:
- ADF (Augmented Dickey-Fuller): StationaritÃ¤tstest
- KPSS (Kwiatkowski-Phillips-Schmidt-Shin): TrendstationaritÃ¤t

Interpretation:
- p-Werte und Teststatistiken
- Kritische Werte fÃ¼r verschiedene Signifikanzniveaus
- Automatische StationaritÃ¤tsbewertung
```

### 7. **Datenanreicherung** (`tabs/advanced_forecast.py`)
```python
Externe Datenquellen:
- Yahoo Finance API: Aktienindizes, Rohstoffpreise
- FRED API: Wirtschaftsindikatoren, ZinssÃ¤tze
- Custom Data Upload: Benutzerdefinierte Zeitreihen

Analysemethoden:
- Pearson-Korrelation
- Granger-KausalitÃ¤tstests  
- Lead-Lag-Analyse
- Cross-Korrelationsfunktionen

Datenverarbeitung:
- Automatische Datumsalignment
- Missing-Data-Interpolation
- Normalisierung und Skalierung
```

### 8. **Multivariate Prognose** (`tabs/multivariate_forecast.py`)
```python
Modell-Pipeline:
1. Datenvorverarbeitung:
   - MinMaxScaler fÃ¼r Normalisierung
   - Sequenz-Generierung fÃ¼r LSTM
   - Feature-Engineering fÃ¼r XGBoost

2. Modelltraining:
   - LSTM: Sequential processing mit Dropout
   - XGBoost: Gradient boosting mit Hyperparameter-Tuning
   - Cross-Validation fÃ¼r Robustheit

3. Prognose-Generierung:
   - Multi-Step-Ahead Forecasting
   - Uncertainty Quantification
   - Konfidenzintervalle

4. Modell-Persistierung:
   - Serialisierung trainierter Modelle
   - Scaler-State-Speicherung
   - Metadaten fÃ¼r Reproduzierbarkeit
```

### 9. **Komplexe Prognose** (`tabs/forecast_complex.py`)
```python
Funktionen:
- Verwendung gespeicherter Modelle aus multivariate_forecast
- Batch-Prognosen fÃ¼r mehrere Produkte
- Ensemble-Methoden (Durchschnitt mehrerer Modelle)
- Automatische Modell-Selection basierend auf Validierungsmetriken

Workflow:
1. Lade gespeicherte Modelle und Scaler
2. Bereite neue Eingabedaten vor
3. Generiere Prognosen mit UnsicherheitsschÃ¤tzung
4. Aggregiere Ensemble-Ergebnisse
5. Visualisiere und exportiere Resultate
```

---

## API-Integrationen

### Yahoo Finance Integration (`tabs/advanced/api_fetch.py`)
```python
VerfÃ¼gbare Datenquellen:
- Aktienindizes: S&P 500, DAX, Nikkei, etc.
- Rohstoffe: Ã–l, Gold, Silber, Kupfer
- WÃ¤hrungen: EUR/USD, GBP/USD, JPY/USD
- KryptowÃ¤hrungen: Bitcoin, Ethereum

Datenformate:
- OHLC (Open, High, Low, Close)
- Volumen
- Adjustierte Schlusskurse
- Historische Daten bis zu 10 Jahre
```

### FRED API Integration
```python
Wirtschaftsindikatoren:
- GDP Growth Rate
- Inflation Rate (CPI)
- Unemployment Rate
- Interest Rates (Federal Funds Rate)
- Industrial Production Index
- Consumer Confidence Index

Abruf-Parameter:
- Automatische Datumsbereich-Anpassung
- Verschiedene Frequenzen (tÃ¤glich, monatlich, quartalsweise)
- Missing-Value-Behandlung
```

---

## Performance-Optimierungen

### Caching-Strategien
```python
@st.cache_data
def load_and_process_data():
    # Caching fÃ¼r schwere Datenoperationen
    
@st.cache_resource  
def train_model():
    # Modell-Training nur bei DatenÃ¤nderungen
```

### Memory Management
```python
# Automatische Garbage Collection nach schweren Berechnungen
import gc
gc.collect()

# Selective data loading fÃ¼r groÃŸe DatensÃ¤tze
chunk_size = 10000
for chunk in pd.read_csv(file, chunksize=chunk_size):
    process_chunk(chunk)
```

### Computational Efficiency
```python
# Vectorized operations mit NumPy/Pandas
data.rolling(window=12).mean()  # statt expliziter Schleifen

# Parallelisierung bei Modell-Training
from concurrent.futures import ProcessPoolExecutor
with ProcessPoolExecutor() as executor:
    results = executor.map(train_model, model_configs)
```

---

## Fehlerbehandlung und Robustheit

### Input Validation
```python
def validate_data(df):
    """Comprehensive data validation"""
    if df.empty:
        raise ValueError("DataFrame is empty")
    
    if df.isnull().sum().sum() > len(df) * 0.3:
        st.warning("Data contains >30% missing values")
    
    return df
```

### Graceful Degradation
```python
try:
    advanced_model_result = train_lstm_model(data)
except (MemoryError, TimeoutError):
    st.warning("Falling back to simpler model")
    fallback_result = train_simple_model(data)
```

### User Feedback
```python
# Progress bars fÃ¼r lange Operationen
progress_bar = st.progress(0)
for i, step in enumerate(training_steps):
    execute_step(step)
    progress_bar.progress((i + 1) / len(training_steps))

# Informative Fehlermeldungen
except ValueError as e:
    st.error(f"Data validation failed: {str(e)}")
    st.info("Please check your data format and try again")
```

---

## Entwicklungsrichtlinien

### Code-Organisation
```
Modularer Aufbau:
- Ein Tab = Ein Modul
- Getrennte Modell-Definitionen
- Zentrale Helper-Funktionen
- Klare AbhÃ¤ngigkeitsstruktur
```

### Testing-Strategie
```python
# Unit Tests fÃ¼r Modelle
def test_arima_model():
    model = ARIMAModel(order=(1,1,1))
    data = generate_test_data()
    model.fit(data)
    forecast = model.predict(steps=12)
    assert len(forecast) == 12

# Integration Tests fÃ¼r Datenfluss
def test_upload_to_forecast_pipeline():
    # VollstÃ¤ndiger Workflow-Test
    pass
```

### Deployment-Konfiguration
```python
# Streamlit Config (.streamlit/config.toml)
[server]
maxUploadSize = 200
enableCORS = false

[theme]
primaryColor = "#FF6B6B"
backgroundColor = "#FFFFFF"
secondaryBackgroundColor = "#F0F2F6"
```

---

## Installation und Entwicklungsumgebung

### Systemanforderungen
```
- Python 3.11+
- RAM: 4GB minimum, 8GB empfohlen
- Festplatte: 500MB fÃ¼r AbhÃ¤ngigkeiten
- Internet: FÃ¼r API-Calls und Package-Installation
```

### Development Setup
```bash
# 1. Repository klonen
git clone <repository-url>
cd PrABCast

# 2. Virtual Environment erstellen
python -m venv venv
venv\Scripts\activate  # Windows
source venv/bin/activate  # Linux/macOS

# 3. Dependencies installieren
pip install -r requirements.txt

# 4. Anwendung starten
streamlit run app/layout.py
```

### Produktions-Deployment
```bash
# Docker-basierte Bereitstellung
FROM python:3.11-slim
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . /app
WORKDIR /app
EXPOSE 8501
CMD ["streamlit", "run", "app/layout.py"]
```

---

## Troubleshooting

### HÃ¤ufige Probleme

#### 1. **Python/Streamlit Installation**
```bash
# Problem: streamlit command not found
# LÃ¶sung: Direkter Python-Aufruf
python -m streamlit run app/layout.py

# Problem: Package-Konflikte
# LÃ¶sung: Clean installation
pip uninstall streamlit
pip install streamlit==1.40.1
```

#### 2. **Datenimport-Probleme**
```python
# Problem: CSV encoding errors
# LÃ¶sung: Automatische Encoding-Erkennung in upload.py

# Problem: Datumserkennung fehlschlÃ¤gt
# LÃ¶sung: Manuelle Format-Spezifikation
pd.to_datetime(df['date'], format='%d.%m.%Y')
```

#### 3. **Memory-Issues bei groÃŸen DatensÃ¤tzen**
```python
# Problem: MemoryError bei LSTM-Training
# LÃ¶sung: Batch-Processing und Sampling
sample_data = data.sample(n=min(10000, len(data)))
```

#### 4. **API-Verbindungsfehler**
```python
# Problem: Yahoo Finance Timeout
# LÃ¶sung: Retry-Mechanismus mit Backoff
import time
for attempt in range(3):
    try:
        data = yf.download(ticker)
        break
    except:
        time.sleep(2 ** attempt)
```

---

## Ausblick und Erweiterungen

### Geplante Features
1. **Erweiterte ML-Modelle**: Transformer, LSTM-Attention
2. **Automatisches Feature Engineering**: Lag-Features, Rolling Statistics
3. **Model Explainability**: SHAP Values, Feature Importance
4. **Real-time Monitoring**: Live-Datenfeeds, Alerting
5. **Multi-User Support**: Benutzerverwaltung, Projekt-Spaces

### Technische Verbesserungen
1. **Performance**: Distributed Computing, GPU-Acceleration  
2. **Scalability**: Database Integration, Cloud Deployment
3. **Robustness**: Enhanced Error Handling, Automated Testing
4. **Usability**: Guided Workflows, Interactive Tutorials

---

## Kontakt und Support

**Entwicklungsteam:**
- RIF - Institut fÃ¼r Forschung und Transfer
- Institut fÃ¼r Produktionssysteme (IPS), TU Dortmund

**Technische UnterstÃ¼tzung:**
- GitHub Issues fÃ¼r Bug-Reports
- Dokumentation und Wiki fÃ¼r Benutzeranleitungen
- Community Forum fÃ¼r Diskussionen

**Projektwebsite:** [IPS Forschungsprojekte](https://ips.mb.tu-dortmund.de/forschen-beraten/forschungsprojekte/prabcast/)

---

*Letzte Aktualisierung: Oktober 2024*
*Version: 2.0*